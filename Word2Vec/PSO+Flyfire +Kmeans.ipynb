{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78763117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8a2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdb3b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Devendra\n",
      "[nltk_data]     Nemade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Devendra\n",
      "[nltk_data]     Nemade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize stopwords, stemmer, and punctuation set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_set = set(string.punctuation)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b9831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lowercase, remove punctuation, remove stopwords, and stem\n",
    "    processed_tokens = [stemmer.stem(word.lower()) for word in tokens if word.lower() not in stop_words and word not in punctuation_set]\n",
    "    # Re-join processed tokens into a single string\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9e4daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the text data...\n",
      "Loading Word2Vec model...\n",
      "Obtaining Word2Vec embeddings for each document...\n"
     ]
    }
   ],
   "source": [
    "processed_data = [preprocess_text(doc) for doc in newsgroups_dataset.data]\n",
    "# Use TfidfVectorizer to convert the raw text into TF-IDF features\n",
    "print(\"Vectorizing the text data...\")\n",
    "import gensim.downloader as api\n",
    "from sklearn.decomposition import PCA\n",
    "# Load Word2Vec model trained on Google News dataset\n",
    "print(\"Loading Word2Vec model...\")\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Define a function to get the vector representation of a document\n",
    "def document_vector(doc):\n",
    "    # Remove punctuation and tokenize the document\n",
    "    tokens = [word.lower() for word in word_tokenize(doc) if word.lower() not in punctuation_set]\n",
    "    # Filter out tokens that are not in the Word2Vec model's vocabulary\n",
    "    tokens = [word for word in tokens if word in word2vec_model.key_to_index]\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    # Calculate the mean of word vectors for tokens in the document\n",
    "    return np.mean(word2vec_model[tokens], axis=0)\n",
    "\n",
    "# Obtain Word2Vec embeddings for each document\n",
    "print(\"Obtaining Word2Vec embeddings for each document...\")\n",
    "word2vec_embeddings = np.array([document_vector(doc) for doc in processed_data])\n",
    "n_components = 100\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "tfidf_matrix_reduced = svd.fit_transform(word2vec_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5befbd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.098119021157757\n",
      "Davies-Bouldin Index: 3.5081965495711147\n",
      "Calinski-Harabasz Index: 1116.635884187646\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Initialize centroids with KMeans\n",
    "def initialize_centroids(data, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "# Compute inertia and assign labels to closest centroid\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
    "\n",
    "def compute_inertia_and_labels(centroids, data, threshold=100000, distance_metric='manhattan'):\n",
    "    # Choose distance metric\n",
    "    if distance_metric == 'euclidean':\n",
    "        distances = euclidean_distances(data, centroids)\n",
    "    elif distance_metric == 'manhattan':\n",
    "        distances = manhattan_distances(data, centroids)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance metric. Choose 'euclidean' or 'manhattan'.\")\n",
    "\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "\n",
    "    # Assign -1 for distances greater than threshold\n",
    "    labels[min_distances > threshold] = -1\n",
    "\n",
    "    # Compute inertia for assigned data points\n",
    "    assigned_data_points = data[labels != -1]\n",
    "    if len(assigned_data_points) > 0:\n",
    "        assigned_labels = labels[labels != -1]\n",
    "        inertia = np.sum((assigned_data_points - centroids[assigned_labels]) ** 2)\n",
    "    else:\n",
    "        inertia = 0\n",
    "\n",
    "    # Calculate additional metrics if needed\n",
    "    num_outliers = np.sum(labels == -1)\n",
    "    average_distance = np.mean(min_distances[labels != -1]) if len(assigned_data_points) > 0 else 0\n",
    "\n",
    "    return inertia, labels\n",
    "\n",
    "\n",
    "# Firefly movement (simplified)\n",
    "def move_towards_brighter(firefly_i, firefly_j, beta0=1, gamma=1):\n",
    "    distance = np.linalg.norm(firefly_i - firefly_j)\n",
    "    beta = beta0 * np.exp(-gamma * distance**2)  # attractiveness\n",
    "    step = beta * (firefly_j - firefly_i)\n",
    "    new_position = firefly_i + step\n",
    "    return new_position\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, data, n_clusters):\n",
    "        self.position = initialize_centroids(data, n_clusters)\n",
    "        self.velocity = np.zeros_like(self.position)\n",
    "        self.best_position = np.copy(self.position)\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "    def update_velocity(self, global_best_position, w=0.5, c1=1, c2=1):\n",
    "        r1, r2 = np.random.rand(), np.random.rand()\n",
    "        self.velocity = w * self.velocity + c1 * r1 * (self.best_position - self.position) + c2 * r2 * (global_best_position - self.position)\n",
    "\n",
    "    def update_position(self, data_bounds):\n",
    "        self.position += self.velocity\n",
    "        # Clip the positions so they stay within the data bounds\n",
    "        self.position = np.clip(self.position, data_bounds[0], data_bounds[1])\n",
    "\n",
    "def pso_firefly_kmeans(data, n_clusters, n_particles=10, max_iter=100, kmeans_interval=20):\n",
    "    particles = [Particle(data, n_clusters) for _ in range(n_particles)]\n",
    "    global_best_score = float('inf')\n",
    "    global_best_position = None\n",
    "\n",
    "    data_bounds = np.array([data.min(axis=0), data.max(axis=0)])\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        for particle in particles:\n",
    "            inertia, _ = compute_inertia_and_labels(particle.position, data)\n",
    "            if inertia < particle.best_score:\n",
    "                particle.best_score = inertia\n",
    "                particle.best_position = np.copy(particle.position)\n",
    "            if inertia < global_best_score:\n",
    "                global_best_score = inertia\n",
    "                global_best_position = np.copy(particle.position)\n",
    "\n",
    "        # PSO update\n",
    "        for particle in particles:\n",
    "            particle.update_velocity(global_best_position)\n",
    "            particle.update_position(data_bounds)\n",
    "\n",
    "        # Firefly update\n",
    "        for particle_i in particles:\n",
    "            for particle_j in particles:\n",
    "                if particle_j.best_score < particle_i.best_score:  # If j is brighter than i\n",
    "                    particle_i.position = move_towards_brighter(particle_i.position, particle_j.best_position)\n",
    "\n",
    "        # KMeans refinement at specified intervals\n",
    "        if i % kmeans_interval == 0 and i != 0:\n",
    "            # Using current global best as initial centroids for KMeans refinement\n",
    "            kmeans = KMeans(n_clusters=n_clusters, init=global_best_position, n_init=1)\n",
    "            kmeans.fit(data)\n",
    "            refined_centroids = kmeans.cluster_centers_\n",
    "\n",
    "            # Update global best if KMeans refinement improves inertia\n",
    "            refined_inertia, _ = compute_inertia_and_labels(refined_centroids, data)\n",
    "            if refined_inertia < global_best_score:\n",
    "                global_best_score = refined_inertia\n",
    "                global_best_position = refined_centroids\n",
    "\n",
    "    final_labels = compute_inertia_and_labels(global_best_position, data)[1]\n",
    "    return global_best_position, final_labels, global_best_score\n",
    "\n",
    "# Assuming 'data' is your dataset and 'n_clusters' is the desired number of clusters\n",
    "best_centroids, labels, best_score = pso_firefly_kmeans(tfidf_matrix_reduced, n_clusters=2)\n",
    "# print(\"Best Score (Inertia):\", best_score)\n",
    "# Calculate and print the clustering metrics\n",
    "silhouette = silhouette_score(tfidf_matrix_reduced, labels)\n",
    "davies_bouldin = davies_bouldin_score(tfidf_matrix_reduced, labels)\n",
    "calinski_harabasz = calinski_harabasz_score(tfidf_matrix_reduced, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabdb95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10264962470270755\n",
      "0.11871721304572484\n",
      "0.08922227379824992\n",
      "0.07253692050702737\n",
      "0.06258150524351398\n",
      "0.06200209756719426\n",
      "0.06340293400821177\n",
      "0.0648279340607857\n",
      "0.06438829643450018\n",
      "0.05243607528426227\n",
      "0.05184439978287381\n",
      "0.04889785104052181\n",
      "0.04999251027172027\n",
      "0.047234045182990805\n",
      "0.048931932632436725\n",
      "0.039566857308263766\n",
      "0.04012699219908922\n",
      "0.035194029245792235\n",
      "0.0427948490775438\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "for n_clusters in range(2, 21):\n",
    "    # Run PSO-KMeans with the current number of clusters\n",
    "    kmeans = KMeans(n_clusters)\n",
    "    kmeans.fit(tfidf_matrix_reduced)\n",
    "    labels=kmeans.labels_\n",
    "\n",
    "    # Calculate metrics\n",
    "    silhouette = silhouette_score(tfidf_matrix_reduced, labels)\n",
    "    davies_bouldin = davies_bouldin_score(tfidf_matrix_reduced,labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(tfidf_matrix_reduced,labels)\n",
    "\n",
    "    # Store metrics\n",
    "    silhouette_scores.append(silhouette)\n",
    "    print(silhouette)\n",
    "    davies_bouldin_scores.append(davies_bouldin)\n",
    "    calinski_harabasz_scores.append(calinski_harabasz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48263cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>silhouette_scores</th>\n",
       "      <th>davies_bouldin_scores</th>\n",
       "      <th>calinski_harabasz_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.102650</td>\n",
       "      <td>3.470357</td>\n",
       "      <td>1140.294888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.118717</td>\n",
       "      <td>2.581191</td>\n",
       "      <td>1059.108398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089222</td>\n",
       "      <td>2.571377</td>\n",
       "      <td>1015.984084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.072537</td>\n",
       "      <td>2.792525</td>\n",
       "      <td>915.098235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.062582</td>\n",
       "      <td>2.860597</td>\n",
       "      <td>814.037854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.062002</td>\n",
       "      <td>2.873105</td>\n",
       "      <td>740.953090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.063403</td>\n",
       "      <td>2.744316</td>\n",
       "      <td>684.631806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.064828</td>\n",
       "      <td>2.689176</td>\n",
       "      <td>639.398911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.064388</td>\n",
       "      <td>2.743545</td>\n",
       "      <td>600.530553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.052436</td>\n",
       "      <td>2.814236</td>\n",
       "      <td>568.794515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.051844</td>\n",
       "      <td>2.683106</td>\n",
       "      <td>540.718864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.048898</td>\n",
       "      <td>2.689233</td>\n",
       "      <td>518.440775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.049993</td>\n",
       "      <td>2.725054</td>\n",
       "      <td>494.255469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.047234</td>\n",
       "      <td>2.815004</td>\n",
       "      <td>471.270902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.048932</td>\n",
       "      <td>2.664872</td>\n",
       "      <td>453.978025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.039567</td>\n",
       "      <td>2.801051</td>\n",
       "      <td>433.804347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.040127</td>\n",
       "      <td>2.668424</td>\n",
       "      <td>421.623473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.035194</td>\n",
       "      <td>2.726182</td>\n",
       "      <td>406.079603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.042795</td>\n",
       "      <td>2.718827</td>\n",
       "      <td>391.586114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    silhouette_scores  davies_bouldin_scores  calinski_harabasz_scores\n",
       "2            0.102650               3.470357               1140.294888\n",
       "3            0.118717               2.581191               1059.108398\n",
       "4            0.089222               2.571377               1015.984084\n",
       "5            0.072537               2.792525                915.098235\n",
       "6            0.062582               2.860597                814.037854\n",
       "7            0.062002               2.873105                740.953090\n",
       "8            0.063403               2.744316                684.631806\n",
       "9            0.064828               2.689176                639.398911\n",
       "10           0.064388               2.743545                600.530553\n",
       "11           0.052436               2.814236                568.794515\n",
       "12           0.051844               2.683106                540.718864\n",
       "13           0.048898               2.689233                518.440775\n",
       "14           0.049993               2.725054                494.255469\n",
       "15           0.047234               2.815004                471.270902\n",
       "16           0.048932               2.664872                453.978025\n",
       "17           0.039567               2.801051                433.804347\n",
       "18           0.040127               2.668424                421.623473\n",
       "19           0.035194               2.726182                406.079603\n",
       "20           0.042795               2.718827                391.586114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'silhouette_scores':silhouette_scores,'davies_bouldin_scores': davies_bouldin_scores,\n",
    "        'calinski_harabasz_scores': calinski_harabasz_scores}\n",
    "df=pd.DataFrame(data,index=range(2, 21))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc083a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Word2vec_PSO+firefly_Scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2020c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
